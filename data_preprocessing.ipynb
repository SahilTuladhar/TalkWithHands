{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing Final Dataset from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "tqdm.pandas() #initialize tqdm for pandas\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import cudf.pandas\n",
    "# cudf.pandas.install()\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing connection \n",
    "\n",
    "connection = MongoClient('localhost' , 27017)\n",
    "db  = connection['mydb']\n",
    "collection = db['Sign_Language_Final_Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data\n",
    "\n",
    "cursor = collection.find({})\n",
    "final_sign_df = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>bbox</th>\n",
       "      <th>fps</th>\n",
       "      <th>frame_end</th>\n",
       "      <th>frame_start</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>signer_id</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "      <th>url</th>\n",
       "      <th>variation_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>is_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>671b7bc2c6201c92805b4f99</td>\n",
       "      <td>book</td>\n",
       "      <td>[385, 37, 885, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>aslbrick</td>\n",
       "      <td>train</td>\n",
       "      <td>http://aslbricks.org/New/ASL-Videos/book.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_69241</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>671b7bc2c6201c92805b4f9a</td>\n",
       "      <td>book</td>\n",
       "      <td>[462, 44, 949, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>signschool</td>\n",
       "      <td>train</td>\n",
       "      <td>https://signstock.blob.core.windows.net/signsc...</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07069</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>671b7bc2c6201c92805b4f9b</td>\n",
       "      <td>book</td>\n",
       "      <td>[234, 17, 524, 414]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>startasl</td>\n",
       "      <td>train</td>\n",
       "      <td>https://s3-us-west-1.amazonaws.com/files.start...</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07068</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>671b7bc2c6201c92805b4f9c</td>\n",
       "      <td>book</td>\n",
       "      <td>[131, 26, 526, 480]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>59</td>\n",
       "      <td>asldeafined</td>\n",
       "      <td>train</td>\n",
       "      <td>https://media.asldeafined.com/vocabulary/14666...</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07070</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>671b7bc2c6201c92805b4f9d</td>\n",
       "      <td>book</td>\n",
       "      <td>[162, 54, 528, 400]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>aslsearch</td>\n",
       "      <td>val</td>\n",
       "      <td>http://www.aslsearch.com/signs/videos/book.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07099</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id gloss                 bbox  fps  frame_end  \\\n",
       "0  671b7bc2c6201c92805b4f99  book  [385, 37, 885, 720]   25         -1   \n",
       "1  671b7bc2c6201c92805b4f9a  book  [462, 44, 949, 720]   25         -1   \n",
       "2  671b7bc2c6201c92805b4f9b  book  [234, 17, 524, 414]   25         -1   \n",
       "3  671b7bc2c6201c92805b4f9c  book  [131, 26, 526, 480]   25         -1   \n",
       "4  671b7bc2c6201c92805b4f9d  book  [162, 54, 528, 400]   25         -1   \n",
       "\n",
       "   frame_start  instance_id  signer_id       source  split  \\\n",
       "0            1            0        118     aslbrick  train   \n",
       "1            1           10         31   signschool  train   \n",
       "2            1           17         36     startasl  train   \n",
       "3            1           22         59  asldeafined  train   \n",
       "4            1           24         12    aslsearch    val   \n",
       "\n",
       "                                                 url  variation_id  \\\n",
       "0       http://aslbricks.org/New/ASL-Videos/book.mp4             0   \n",
       "1  https://signstock.blob.core.windows.net/signsc...             0   \n",
       "2  https://s3-us-west-1.amazonaws.com/files.start...             0   \n",
       "3  https://media.asldeafined.com/vocabulary/14666...             0   \n",
       "4     http://www.aslsearch.com/signs/videos/book.mp4             0   \n",
       "\n",
       "     video_id  is_available  \n",
       "0  v_id_69241          True  \n",
       "1  v_id_07069          True  \n",
       "2  v_id_07068          True  \n",
       "3  v_id_07070          True  \n",
       "4  v_id_07099          True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sign_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #Removing v_id_ from v_id so that the path can be matched up\n",
    "final_sign_df['video_id'] = final_sign_df['video_id'].apply(lambda id : id.replace('v_id_',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>bbox</th>\n",
       "      <th>fps</th>\n",
       "      <th>frame_end</th>\n",
       "      <th>frame_start</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>signer_id</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "      <th>url</th>\n",
       "      <th>variation_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>is_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>671b7bc2c6201c92805b4f99</td>\n",
       "      <td>book</td>\n",
       "      <td>[385, 37, 885, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>aslbrick</td>\n",
       "      <td>train</td>\n",
       "      <td>http://aslbricks.org/New/ASL-Videos/book.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>69241</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>671b7bc2c6201c92805b4f9a</td>\n",
       "      <td>book</td>\n",
       "      <td>[462, 44, 949, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>signschool</td>\n",
       "      <td>train</td>\n",
       "      <td>https://signstock.blob.core.windows.net/signsc...</td>\n",
       "      <td>0</td>\n",
       "      <td>07069</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>671b7bc2c6201c92805b4f9b</td>\n",
       "      <td>book</td>\n",
       "      <td>[234, 17, 524, 414]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>startasl</td>\n",
       "      <td>train</td>\n",
       "      <td>https://s3-us-west-1.amazonaws.com/files.start...</td>\n",
       "      <td>0</td>\n",
       "      <td>07068</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>671b7bc2c6201c92805b4f9c</td>\n",
       "      <td>book</td>\n",
       "      <td>[131, 26, 526, 480]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>59</td>\n",
       "      <td>asldeafined</td>\n",
       "      <td>train</td>\n",
       "      <td>https://media.asldeafined.com/vocabulary/14666...</td>\n",
       "      <td>0</td>\n",
       "      <td>07070</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>671b7bc2c6201c92805b4f9d</td>\n",
       "      <td>book</td>\n",
       "      <td>[162, 54, 528, 400]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>aslsearch</td>\n",
       "      <td>val</td>\n",
       "      <td>http://www.aslsearch.com/signs/videos/book.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>07099</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>671b7bc2c6201c92805b7e60</td>\n",
       "      <td>wheelchair</td>\n",
       "      <td>[39, 13, 248, 192]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>signingsavvy</td>\n",
       "      <td>train</td>\n",
       "      <td>https://www.signingsavvy.com/signs/mp4/5/5233.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>63047</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>671b7bc2c6201c92805b7e61</td>\n",
       "      <td>wheelchair</td>\n",
       "      <td>[163, 62, 625, 400]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>aslsearch</td>\n",
       "      <td>train</td>\n",
       "      <td>http://www.aslsearch.com/signs/videos/wheelcha...</td>\n",
       "      <td>0</td>\n",
       "      <td>63050</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11977</th>\n",
       "      <td>671b7bc2c6201c92805b7e62</td>\n",
       "      <td>whistle</td>\n",
       "      <td>[76, 17, 236, 240]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>spreadthesign</td>\n",
       "      <td>train</td>\n",
       "      <td>https://media.spreadthesign.com/video/mp4/13/9...</td>\n",
       "      <td>0</td>\n",
       "      <td>63186</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11978</th>\n",
       "      <td>671b7bc2c6201c92805b7e63</td>\n",
       "      <td>whistle</td>\n",
       "      <td>[68, 14, 212, 192]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>signingsavvy</td>\n",
       "      <td>train</td>\n",
       "      <td>https://www.signingsavvy.com/signs/mp4/9/9961.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>63188</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11979</th>\n",
       "      <td>671b7bc2c6201c92805b7e64</td>\n",
       "      <td>whistle</td>\n",
       "      <td>[210, 55, 567, 400]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>aslsearch</td>\n",
       "      <td>train</td>\n",
       "      <td>http://www.aslsearch.com/signs/videos/whistle.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>63190</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11980 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _id       gloss                 bbox  fps  \\\n",
       "0      671b7bc2c6201c92805b4f99        book  [385, 37, 885, 720]   25   \n",
       "1      671b7bc2c6201c92805b4f9a        book  [462, 44, 949, 720]   25   \n",
       "2      671b7bc2c6201c92805b4f9b        book  [234, 17, 524, 414]   25   \n",
       "3      671b7bc2c6201c92805b4f9c        book  [131, 26, 526, 480]   25   \n",
       "4      671b7bc2c6201c92805b4f9d        book  [162, 54, 528, 400]   25   \n",
       "...                         ...         ...                  ...  ...   \n",
       "11975  671b7bc2c6201c92805b7e60  wheelchair   [39, 13, 248, 192]   25   \n",
       "11976  671b7bc2c6201c92805b7e61  wheelchair  [163, 62, 625, 400]   25   \n",
       "11977  671b7bc2c6201c92805b7e62     whistle   [76, 17, 236, 240]   25   \n",
       "11978  671b7bc2c6201c92805b7e63     whistle   [68, 14, 212, 192]   25   \n",
       "11979  671b7bc2c6201c92805b7e64     whistle  [210, 55, 567, 400]   25   \n",
       "\n",
       "       frame_end  frame_start  instance_id  signer_id         source  split  \\\n",
       "0             -1            1            0        118       aslbrick  train   \n",
       "1             -1            1           10         31     signschool  train   \n",
       "2             -1            1           17         36       startasl  train   \n",
       "3             -1            1           22         59    asldeafined  train   \n",
       "4             -1            1           24         12      aslsearch    val   \n",
       "...          ...          ...          ...        ...            ...    ...   \n",
       "11975         -1            1            5         11   signingsavvy  train   \n",
       "11976         -1            1            8         12      aslsearch  train   \n",
       "11977         -1            1            2          2  spreadthesign  train   \n",
       "11978         -1            1            4         11   signingsavvy  train   \n",
       "11979         -1            1            6         12      aslsearch  train   \n",
       "\n",
       "                                                     url  variation_id  \\\n",
       "0           http://aslbricks.org/New/ASL-Videos/book.mp4             0   \n",
       "1      https://signstock.blob.core.windows.net/signsc...             0   \n",
       "2      https://s3-us-west-1.amazonaws.com/files.start...             0   \n",
       "3      https://media.asldeafined.com/vocabulary/14666...             0   \n",
       "4         http://www.aslsearch.com/signs/videos/book.mp4             0   \n",
       "...                                                  ...           ...   \n",
       "11975  https://www.signingsavvy.com/signs/mp4/5/5233.mp4             0   \n",
       "11976  http://www.aslsearch.com/signs/videos/wheelcha...             0   \n",
       "11977  https://media.spreadthesign.com/video/mp4/13/9...             0   \n",
       "11978  https://www.signingsavvy.com/signs/mp4/9/9961.mp4             0   \n",
       "11979  http://www.aslsearch.com/signs/videos/whistle.mp4             0   \n",
       "\n",
       "      video_id  is_available  \n",
       "0        69241          True  \n",
       "1        07069          True  \n",
       "2        07068          True  \n",
       "3        07070          True  \n",
       "4        07099          True  \n",
       "...        ...           ...  \n",
       "11975    63047          True  \n",
       "11976    63050          True  \n",
       "11977    63186          True  \n",
       "11978    63188          True  \n",
       "11979    63190          True  \n",
       "\n",
       "[11980 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sign_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11980"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_sign_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Defining Function to Extract certain number of frames from the video \n",
    " - Convert the video into a sequence of frames that can be processed individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame , output_size):\n",
    "\n",
    "   # We do not need to explicitly normalize by using frame /255.0 as when converting values of pixel present in int ranging from 0 to 255 , when converting to tf.float32 it scales the values between [0 , 1]\n",
    "\n",
    "   frame = tf.image.convert_image_dtype(frame , tf.float32)\n",
    "   frame = tf.image.resize_with_pad(frame , *output_size)\n",
    "   return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_frame_values(frame , frame_number):\n",
    "  \n",
    "  if np.all(frame == 0):\n",
    "    \n",
    "    print(f\"zeros at frame : {frame_number}\")\n",
    "  \n",
    "  else:\n",
    "    \n",
    "    if np.any((frame > 0) & (frame <=1)):\n",
    "      \n",
    "      print(f'frame {frame_number} contains valid frames')\n",
    "    \n",
    "    else:\n",
    "      \n",
    "      print(f'frame {frame_number} contains invalid frames')\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video(video_id , n_frames ,v_dict, label, output_size = (224,224) , frame_step = 4 ):\n",
    "  \n",
    "  video_path = rf'C:\\Users\\Sahil\\Desktop\\Talkwithhands dataset\\versions\\5\\videos\\{video_id}.mp4'\n",
    "\n",
    "  result = []\n",
    "  src = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "  #verify if the video path is correct and the video can be opened\n",
    "\n",
    "  if not src.isOpened():\n",
    "    print(f\"Failed to open video: {video_path}\")\n",
    "    return None\n",
    "  \n",
    "  \n",
    "  # Extracting total video frames \n",
    "  video_length  = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "  # Checking whether video has enough frames as required\n",
    "\n",
    "  if need_length > video_length:\n",
    "     \n",
    "     start = 0\n",
    "  \n",
    "  else:\n",
    "    \n",
    "    max_start = video_length - need_length\n",
    "    start = np.random.randint(0 , max_start + 1)\n",
    "\n",
    "  # Setting the video to the starting frame - start\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES , start)\n",
    "\n",
    "  # ret is a boolean that returns TRUE if frame was read successfully and frame is the image data\n",
    "\n",
    " # capturing the first frame\n",
    "  ret , frame = src.read()\n",
    "  result.append(format_frames(frame , output_size))\n",
    "\n",
    " # Capturig further more frames\n",
    "\n",
    "  for i in range(n_frames - 1): # first frame already taken\n",
    "\n",
    "    for _ in range(frame_step):\n",
    "\n",
    "      ret , frame = src.read()\n",
    "    \n",
    "    if ret :\n",
    "\n",
    "      frame = format_frames(frame , output_size)\n",
    "      result.append(frame)\n",
    "\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0])) #If no more frames present it inserts zeros with the same shape as the first frame\n",
    "  \n",
    "  src.release()\n",
    "  \n",
    "  # The [2, 1, 0] indexing reverses the color channels from BGR to RGB (as OpenCV loads images in BGR format, but many libraries expect RGB).\n",
    "\n",
    "  result = np.array(result)[... , [2,1,0]] \n",
    "\n",
    "  dict_element = {\n",
    "    'frames_data' : result,\n",
    "    'label' : label\n",
    "  }\n",
    "\n",
    "  v_dict[video_id] = dict_element \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_video(video_id):\n",
    " \n",
    "#  video_data_dict ={}\n",
    " \n",
    "#  video_path = rf'C:\\Users\\Sahil\\Desktop\\Talkwithhands dataset\\versions\\5\\videos\\{video_id}.mp4'\n",
    "#  cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "#  # Verify if the video path is correct and the video can be opened\n",
    "#  if not cap.isOpened():\n",
    "#     print(f\"Failed to open video: {video_path}\")\n",
    "#     return None\n",
    "\n",
    "#  frames = []\n",
    "\n",
    "#  # Extract Frames \n",
    "\n",
    "#  while True:\n",
    "\n",
    "#     ret , frame = cap.read()\n",
    "\n",
    "#     if not ret:\n",
    "     \n",
    "#          break\n",
    "\n",
    "#     # Resizing the frames \n",
    "\n",
    "#     frame = cv2.resize(frame , (112 , 112))\n",
    "\n",
    "#     #Normalizing the frames \n",
    "#     frame = frame.astype('float32') / 255.0\n",
    "\n",
    "#     frames.append(frame) \n",
    " \n",
    "#  cap.release()\n",
    "# #  video_data_dict[video_id] = current_video_data\n",
    " \n",
    "\n",
    "#  return frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extracting Frames from each video present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Processing In Batches to prvent kernel crashes\n",
    "# all_video_data = [] # list to collect all the batch video data\n",
    "\n",
    "# batch_size = 20\n",
    "\n",
    "# num_batches = len(final_sign_df) // batch_size + 1\n",
    "\n",
    "# for i in tqdm(range(num_batches)):\n",
    "#  batch_df = final_sign_df.iloc[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "#   # Process the batch and collect results\n",
    "#  video_data_batch = batch_df.progress_apply(lambda record: preprocess_video(record['video_id']), axis=1)\n",
    "\n",
    "\n",
    "#  video_batch_dict = [{'video_id' : record['video_id'] , 'label' : record['gloss'] , 'video_data' : [frame.tolist() for frame in data]} \n",
    "#                      for record , data in zip(batch_df.to_dict(orient='records') , video_data_batch)]\n",
    " \n",
    "\n",
    "#  # Append the results to the list\n",
    "#  # all_video_data.append(video_data_batch.tolist()) # convert series to list\n",
    "\n",
    "#  # Clear variables and run garbage collector\n",
    "#  del batch_df, video_data_batch , video_batch_dict\n",
    "#  gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 686/1000 [02:15<01:01,  5.07it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 6220800 bytes in function 'cv::OutOfMemoryError'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m  batch_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Process the batch and and add it to the dictionary\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m  \u001b[43mbatch_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m  \u001b[38;5;66;03m# add current batch dictionary to the list\u001b[39;00m\n\u001b[0;32m     23\u001b[0m  video_frames_dict\u001b[38;5;241m.\u001b[39mappend(batch_dict)\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(record)\u001b[0m\n\u001b[0;32m     15\u001b[0m  batch_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Process the batch and and add it to the dictionary\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m  batch_df\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m record : \u001b[43mframes_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m , axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m )\n\u001b[0;32m     21\u001b[0m  \u001b[38;5;66;03m# add current batch dictionary to the list\u001b[39;00m\n\u001b[0;32m     23\u001b[0m  video_frames_dict\u001b[38;5;241m.\u001b[39mappend(batch_dict)\n",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m, in \u001b[0;36mframes_from_video\u001b[1;34m(video_id, n_frames, v_dict, label, output_size, frame_step)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m): \u001b[38;5;66;03m# first frame already taken\u001b[39;00m\n\u001b[0;32m     45\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frame_step):\n\u001b[1;32m---> 47\u001b[0m     ret , frame \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ret :\n\u001b[0;32m     51\u001b[0m     frame \u001b[38;5;241m=\u001b[39m format_frames(frame , output_size)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 6220800 bytes in function 'cv::OutOfMemoryError'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Intializing empty dict to store each frame as a entry to the dictionary\n",
    "video_frames_dict = []\n",
    "\n",
    "# Extracting frames from videos using batches to prevent kernel crashes\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "num_batches = len(final_sign_df) // batch_size + 1\n",
    "\n",
    "for i in (range(num_batches)):\n",
    "\n",
    "  batch_df = final_sign_df.iloc[i * batch_size : (i + 1)*batch_size ]\n",
    "\n",
    "  # Initialiing a dictionary for the current batch\n",
    "  batch_dict = {}\n",
    "\n",
    " # Process the batch and and add it to the dictionary\n",
    "\n",
    "  batch_df.progress_apply(lambda record : frames_from_video(video_id= record['video_id'] , n_frames = int(15) , v_dict= batch_dict ,label = record['gloss']) , axis = 1 )\n",
    "\n",
    "  # add current batch dictionary to the list\n",
    "\n",
    "  video_frames_dict.append(batch_dict)\n",
    "\n",
    "  # Clear batch dictionary and collected garbage to free memory\n",
    "  batch_dict.clear()\n",
    "  batch_df = None\n",
    "  gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1217/11980 [05:15<46:27,  3.86it/s]  \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 17.2 MiB for an array with shape (3, 30, 224, 224) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m video_frames_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Applying to each video in the dataframe\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mfinal_sign_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvideo_frames_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(record)\u001b[0m\n\u001b[0;32m      2\u001b[0m video_frames_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Applying to each video in the dataframe\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m final_sign_df\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m record : \u001b[43mframes_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvideo_frames_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m , axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m )\n",
      "Cell \u001b[1;32mIn[10], line 61\u001b[0m, in \u001b[0;36mframes_from_video\u001b[1;34m(video_id, n_frames, v_dict, label, output_size, frame_step)\u001b[0m\n\u001b[0;32m     57\u001b[0m src\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# The [2, 1, 0] indexing reverses the color channels from BGR to RGB (as OpenCV loads images in BGR format, but many libraries expect RGB).\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \n\u001b[0;32m     63\u001b[0m dict_element \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframes_data\u001b[39m\u001b[38;5;124m'\u001b[39m : result,\n\u001b[0;32m     65\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m : label\n\u001b[0;32m     66\u001b[0m }\n\u001b[0;32m     68\u001b[0m v_dict[video_id] \u001b[38;5;241m=\u001b[39m dict_element\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 17.2 MiB for an array with shape (3, 30, 224, 224) and data type float32"
     ]
    }
   ],
   "source": [
    "# # Intializing empty dict to store each frame as a entry to the dictionary\n",
    "# video_frames_dict = {}\n",
    "\n",
    "# # Applying to each video in the dataframe\n",
    "\n",
    "# final_sign_df.progress_apply(lambda record : frames_from_video(video_id= record['video_id'] , n_frames = int(30) , v_dict= video_frames_dict ,label = record['gloss']) , axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Applying Pre-processing the each video and then processing them in batches and storing each batch to mongoDB for memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch to MongoDB...\n",
      "An error occurred while inserting to MongoDB: BSONObj size: 39606890 (0x25C5A6A) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('671bad4b587db12f2f498a36'), full error: {'ok': 0.0, 'errmsg': \"BSONObj size: 39606890 (0x25C5A6A) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('671bad4b587db12f2f498a36')\", 'code': 10334, 'codeName': 'BSONObjectTooLarge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.58it/s]t]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch to MongoDB...\n",
      "An error occurred while inserting to MongoDB: BSONObj size: 46472077 (0x2C51B8D) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('671bad5c587db12f2f498a4b'), full error: {'ok': 0.0, 'errmsg': \"BSONObj size: 46472077 (0x2C51B8D) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: ObjectId('671bad5c587db12f2f498a4b')\", 'code': 10334, 'codeName': 'BSONObjectTooLarge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.61it/s]t]\n",
      "  0%|          | 2/600 [00:36<2:59:31, 18.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m video_data_batch \u001b[38;5;241m=\u001b[39m batch_df\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m record: preprocess_video(record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_id\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Prepate batch data to be inserted to MongoDB\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m video_batch_dict \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_id\u001b[39m\u001b[38;5;124m'\u001b[39m : record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_id\u001b[39m\u001b[38;5;124m'\u001b[39m] , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m : record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgloss\u001b[39m\u001b[38;5;124m'\u001b[39m] , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_data\u001b[39m\u001b[38;5;124m'\u001b[39m : [frame\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m data]} \n\u001b[0;32m     18\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m record , data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_df\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m) , video_data_batch)]\n\u001b[0;32m     20\u001b[0m  \u001b[38;5;66;03m# Insert each batch into MongoDB collection\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11980 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2086/11980 [04:31<17:06,  9.64it/s]"
     ]
    }
   ],
   "source": [
    "# # Testing the above function \n",
    "\n",
    "# video_data_list = final_sign_df.progress_apply(lambda record : preprocess_video(record['video_id'] , record['gloss'])  , axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Converting Into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_frames_df = pd.DataFrame.from_dict(vid_data , orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>frames</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69241</td>\n",
       "      <td>[[[[0.37254903 0.38039216 0.3882353 ], [0.3686...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                             frames label\n",
       "0  69241  [[[[0.37254903 0.38039216 0.3882353 ], [0.3686...  book"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_frames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "  'student1' : {\n",
    "   'name' : 'sahil',\n",
    "   'age' : 18\n",
    "  } ,\n",
    "\n",
    "  'student2' : {\n",
    "   'name' : 'salina',\n",
    "   'age' : 17\n",
    "  } ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.DataFrame.from_dict(dict , orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>student1</td>\n",
       "      <td>sahil</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>student2</td>\n",
       "      <td>salina</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index    name  age\n",
       "0  student1   sahil   18\n",
       "1  student2  salina   17"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
