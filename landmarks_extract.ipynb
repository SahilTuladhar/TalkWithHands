{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting frames and landmarks from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %load_ext cudf.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from pymongo import MongoClient\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "# from pandarallel import pandarallel\n",
    "# import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'mydb'\n",
    "input_collection_name = 'Sign_Language_Final_Data'\n",
    "output_collection_name = 'Sign_Language_Processed_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = MongoClient('localhost', 27017)\n",
    "db = connection[db_name]\n",
    "input_collection = db[input_collection_name]\n",
    "cursor = input_collection.find({})\n",
    "df = pd.DataFrame(list(cursor))\n",
    "connection.close()  # Close the initial connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['_id', 'source', 'url'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['video_id'] = df['video_id'].str.replace('v_id_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fps'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frame_start'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frame_end'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['split'] == 'train').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['split'] == 'test').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['split'] == 'val').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gloss'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    return frame_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Landmark points\n",
    "- It is a really important step which allows use to generated similar landmark pssotion values for same gestures performed in different orientation by introducing\n",
    "    - Position Invariance\n",
    "    - Scale Invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_landmarks(x , y, z , x_min , y_min , x_max , y_max , f_width , f_height):\n",
    "  \n",
    "  # Handle different types of landmark inputs\n",
    "  # if hasattr(landmarks, 'landmark'):\n",
    "  #       landmark_list = [lm for lm in landmarks.landmark]  # For face_landmarks\n",
    "  # else:\n",
    "  #       landmark_list = landmarks  # For hand landmarks that are already a list\n",
    " \n",
    "\n",
    "  normalized = []\n",
    "\n",
    "  width = x_max - x_min\n",
    "  height = y_max - y_min\n",
    "    \n",
    "  norm_x = ((x *f_width) - x_min)/(width)\n",
    "  norm_y = ((y *f_height) - y_min)/(height)\n",
    "  norm_z = z\n",
    "\n",
    "  normalized.append((norm_x , norm_y , norm_z))\n",
    "  \n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame , output_size , x_min , y_min , x_max , y_max , f_width , f_height):\n",
    "  \n",
    "  n_width , n_height = output_size\n",
    "\n",
    "  # Along with resizing the frames we need to rescale the bounding box values\n",
    "\n",
    "  width_scale_factor = (n_width / f_width )\n",
    "  height_scale_factor = (n_height / f_height)\n",
    "\n",
    "  n_x_min = x_min * width_scale_factor\n",
    "  n_y_min =  y_min * height_scale_factor\n",
    "  n_x_max = x_max * width_scale_factor\n",
    "  n_y_max = y_max * height_scale_factor\n",
    "\n",
    "  # frame = tf.image.convert_image_dtype(frame , tf.float32)\n",
    "\n",
    "  frame = cv2.resize(frame , output_size)\n",
    "\n",
    "  return frame ,  n_x_min , n_y_min , n_x_max , n_y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(row):\n",
    "    video_path = f'./kaggle-dataset/videos/{row.video_id}.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)  # Open video for each call\n",
    "\n",
    "    try:\n",
    "        # Get total frame count for the video\n",
    "        total_frames = count_frames(video_path)\n",
    "        frame_start = row.frame_start\n",
    "        frame_end = total_frames - 1 if row.frame_end == -1 else row.frame_end\n",
    "\n",
    "        x_min, y_min, x_max, y_max = row.bbox\n",
    "        skip_interval = int(row.fps / 10)\n",
    "\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        # Initialize list to hold landmarks for this row\n",
    "        landmarks_sequence = []\n",
    "        cropped_width = x_max - x_min\n",
    "        cropped_height = y_max - y_min\n",
    "\n",
    "        # sequence_length = 30\n",
    "\n",
    "        # skip_frames_window = max(int(total_frames/sequence_length) , 1)\n",
    "\n",
    "\n",
    "        with mp_holistic.Holistic(static_image_mode=False, model_complexity=2) as holistic:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start)\n",
    "\n",
    "            while cap.isOpened() and frame_start <= frame_end:\n",
    "                # for frame_counter in range(sequence_length):\n",
    "                    # current_frame = cap.set(cv2.CAP_PROP_POS_FRAMES , frame_counter * skip_frames_window)\n",
    "                    current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "                    \n",
    "                    if current_frame > frame_end:\n",
    "                        break\n",
    "\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    frame , n_x_min , n_y_min , n_x_max , n_y_max = format_frames(frame , output_size=(224 , 224) , x_min=x_min ,y_min= y_min ,x_max= x_max , y_max=y_max ,f_width= frame_width ,f_height= frame_height)\n",
    "\n",
    "                    \n",
    "                    cropped_frame = frame[int(n_y_min):int(n_y_max), int(n_x_min):int(n_x_max)]\n",
    "                    image_rgb = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB)\n",
    "                    holistic.image_dimensions = (cropped_width, cropped_height)\n",
    "                    results = holistic.process(image_rgb)\n",
    "\n",
    "                    # Create a landmarks dictionary without storing None values\n",
    "                    frame_landmarks = {}\n",
    "                    if results.face_landmarks:\n",
    "                        frame_landmarks[\"face\"] = [normalize_landmarks(lm.x, lm.y, lm.z , n_x_min , n_y_min , n_x_max , n_y_max , f_width= 224, f_height=224) for lm in results.face_landmarks.landmark]\n",
    "                    \n",
    "                    else: \n",
    "                        frame_landmarks['face'] = [( 0 , 0 , 0) for _ in range(468)]\n",
    "\n",
    "                    if results.left_hand_landmarks:\n",
    "                        frame_landmarks[\"left_hand\"] = [normalize_landmarks(lm.x, lm.y, lm.z , n_x_min , n_y_min , n_x_max , n_y_max , f_width=224 , f_height=224) for lm in results.left_hand_landmarks.landmark]\n",
    "\n",
    "                    else:\n",
    "                        frame_landmarks['left_hand'] = [( 0 , 0 , 0) for _ in range(21)]\n",
    "\n",
    "                    if results.right_hand_landmarks:\n",
    "                        frame_landmarks[\"right_hand\"] = [normalize_landmarks(lm.x, lm.y, lm.z , n_x_min , n_y_min , n_x_max , y_max , f_width=224 , f_height=224) for lm in results.right_hand_landmarks.landmark]\n",
    "                    \n",
    "                    else:\n",
    "                        frame_landmarks['right_hand'] = [( 0 , 0 , 0) for _ in range(21)]\n",
    "                    \n",
    "                    landmarks_sequence.append(frame_landmarks)\n",
    "                    frame_start += skip_interval\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start)\n",
    "\n",
    "                # while len(landmarks_sequence) < sequence_length:\n",
    "\n",
    "                #     zero_landmarks = { \n",
    "                #         'face' : [(0 , 0 , 0) for _ in range(468)],\n",
    "                #         'left_hand' : [(0 , 0 , 0) for _ in range(21)],\n",
    "                #         'right_hand' : [(0 , 0 , 0) for _ in range(21) ]\n",
    "                    \n",
    "                #     }\n",
    "\n",
    "                #     landmarks_sequence.append(zero_landmarks)\n",
    "    finally:\n",
    "        cap.release()  # Ensure cap is released no matter what\n",
    "\n",
    "    return {\"gloss\": row.gloss, \"instance_id\": row.instance_id, \"landmarks_sequence\": landmarks_sequence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import logging\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "# tqdm.pandas(desc=\"Processing videos\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# some_rows = df.iloc[[0, 1, 2, 3, 4,]]\n",
    "\n",
    "# n_jobs = -1\n",
    "\n",
    "# results = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "#     delayed(extract_landmarks)(row) for row in tqdm(df.itertuples(), total=len(df))\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "n_jobs = -1\n",
    "\n",
    "for start_idx in range(0, len(df), batch_size):\n",
    "    end_idx = min(start_idx + batch_size, len(df))\n",
    "    batch = df.iloc[start_idx:end_idx].copy()  # Copy batch to avoid reference issues\n",
    "\n",
    "    # Create and run the Parallel instance for the current batch\n",
    "    with Parallel(n_jobs=n_jobs, backend=\"multiprocessing\") as parallel:\n",
    "        batch_results = parallel(\n",
    "            delayed(extract_landmarks)(row) for _, row in tqdm(batch.iterrows(), total=len(batch))\n",
    "        )\n",
    "    \n",
    "    # Re-establish MongoDB connection for each batch\n",
    "    connection = MongoClient('localhost', 27017)\n",
    "    db = connection[db_name]\n",
    "    output_collection = db[output_collection_name]\n",
    "\n",
    "    # Insert batch results into MongoDB\n",
    "    output_collection.insert_many(batch_results)\n",
    "\n",
    "    # Close MongoDB connection after each batch\n",
    "    connection.close()\n",
    "\n",
    "    # Clear batch and batch results from memory\n",
    "    del batch, batch_results, parallel  # Explicitly delete the parallel instance\n",
    "    gc.collect()  # Explicitly trigger garbage collection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def process_and_clear(row):\n",
    "#     result = extract_landmarks(row)\n",
    "#     del row  # Free up memory for each row after processing\n",
    "#     return result\n",
    "\n",
    "# # Run parallel processing with memory-efficient approach\n",
    "# results = []\n",
    "# for res in Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "#     delayed(process_and_clear)(row) for row in tqdm(df.itertuples(), total=len(df))\n",
    "# ):\n",
    "#     results.append(res)\n",
    "#     del res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dask_df = dd.from_pandas(some_rows, npartitions=14)\n",
    "\n",
    "# def apply_extract_landmarks(df):\n",
    "#     return df.apply(extract_landmarks, axis=1)\n",
    "\n",
    "\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# with ProgressBar():\n",
    "#     result = dask_df.map_partitions(apply_extract_landmarks).compute()\n",
    "\n",
    "\n",
    "\n",
    "# landmarks_data = some_rows.parallel_apply(extract_landmarks, axis = 1)\n",
    "\n",
    "\n",
    "# def parallel_extract_landmarks(df):\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         results = list(executor.map(extract_landmarks, [row for _, row in df.iterrows()]))\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "# results_df = parallel_extract_landmarks(some_rows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# some_rows.progress_apply(extract_landmarks, axis = 1)\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "#     futures = {executor.submit(extract_landmarks, row): index for index, row in some_rows.iterrows()}\n",
    "#     landmarks_data = []\n",
    "\n",
    "#     for future in tqdm(futures, desc=\"Collecting results\"):\n",
    "#         result = future.result()\n",
    "#         landmarks_data.append(result)\n",
    "#         # logging.info(f\"Collected landmarks for gloss: {result[0]}, instance_id: {result[1]}\")\n",
    "\n",
    "# landmarks_df = pd.DataFrame(landmarks_data, columns=[\"gloss\", \"instance_id\", \"landmarks_sequence\"])\n",
    "\n",
    "# # logging.info(landmarks_df)\n",
    "\n",
    "# merged_df = pd.merge(df, landmarks_df, on=['gloss', 'instance_id'], how='inner')\n",
    "\n",
    "# # Release all video captures after processing\n",
    "# for cap in video_capture_cache.values():\n",
    "#     cap.release()\n",
    "\n",
    "\n",
    "# holistic.close()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_df = pd.DataFrame(results, columns=[\"gloss\",\"instance_id\", \"landmarks_sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, landmarks_df, on=['gloss', 'instance_id'], how='inner')  # You can change 'inner' to 'outer', 'left', or 'right' depending on the merge type you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# with mp_holistic.Holistic(static_image_mode=False, \n",
    "#                            model_complexity=2, \n",
    "#                            enable_segmentation=True,\n",
    "#                            min_detection_confidence=0.5,\n",
    "#                            min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "#         ret,frame = cap.read()\n",
    "#         if not ret:\n",
    "#             print(\"Ignoring empty camera frame\")\n",
    "#             continue\n",
    "\n",
    "#         frame = cv2.flip(frame, 1)\n",
    "        \n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         image.flags.writeable = False\n",
    "\n",
    "#         results = holistic.process(image)\n",
    "\n",
    "#         image.flags.writeable = True\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#         if results.face_landmarks:\n",
    "#              mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)\n",
    "#         # if results.pose_landmarks:\n",
    "#         #      mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "#         if results.left_hand_landmarks:\n",
    "#              mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "#         if results.right_hand_landmarks:\n",
    "#              mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#         cv2.imshow('TEST', image)\n",
    "\n",
    "#         if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "#                 break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
