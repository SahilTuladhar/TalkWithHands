{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting frames and landmarks from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "holistic = mp_holistic.Holistic(static_image_mode = False,\n",
    "                                model_complexity = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = MongoClient('localhost', 27017)\n",
    "db = connection['mydb']\n",
    "collection = db['Sign_Language_Final_Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = collection.find({})\n",
    "df = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>bbox</th>\n",
       "      <th>fps</th>\n",
       "      <th>frame_end</th>\n",
       "      <th>frame_start</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>signer_id</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "      <th>url</th>\n",
       "      <th>variation_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>is_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>671b7bc2c6201c92805b4f99</td>\n",
       "      <td>book</td>\n",
       "      <td>[385, 37, 885, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>aslbrick</td>\n",
       "      <td>train</td>\n",
       "      <td>http://aslbricks.org/New/ASL-Videos/book.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_69241</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>671b7bc2c6201c92805b4f9a</td>\n",
       "      <td>book</td>\n",
       "      <td>[462, 44, 949, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>signschool</td>\n",
       "      <td>train</td>\n",
       "      <td>https://signstock.blob.core.windows.net/signsc...</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07069</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>671b7bc2c6201c92805b4f9b</td>\n",
       "      <td>book</td>\n",
       "      <td>[234, 17, 524, 414]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>startasl</td>\n",
       "      <td>train</td>\n",
       "      <td>https://s3-us-west-1.amazonaws.com/files.start...</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07068</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>671b7bc2c6201c92805b4f9c</td>\n",
       "      <td>book</td>\n",
       "      <td>[131, 26, 526, 480]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>59</td>\n",
       "      <td>asldeafined</td>\n",
       "      <td>train</td>\n",
       "      <td>https://media.asldeafined.com/vocabulary/14666...</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07070</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>671b7bc2c6201c92805b4f9d</td>\n",
       "      <td>book</td>\n",
       "      <td>[162, 54, 528, 400]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>aslsearch</td>\n",
       "      <td>val</td>\n",
       "      <td>http://www.aslsearch.com/signs/videos/book.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07099</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id gloss                 bbox  fps  frame_end  \\\n",
       "0  671b7bc2c6201c92805b4f99  book  [385, 37, 885, 720]   25         -1   \n",
       "1  671b7bc2c6201c92805b4f9a  book  [462, 44, 949, 720]   25         -1   \n",
       "2  671b7bc2c6201c92805b4f9b  book  [234, 17, 524, 414]   25         -1   \n",
       "3  671b7bc2c6201c92805b4f9c  book  [131, 26, 526, 480]   25         -1   \n",
       "4  671b7bc2c6201c92805b4f9d  book  [162, 54, 528, 400]   25         -1   \n",
       "\n",
       "   frame_start  instance_id  signer_id       source  split  \\\n",
       "0            1            0        118     aslbrick  train   \n",
       "1            1           10         31   signschool  train   \n",
       "2            1           17         36     startasl  train   \n",
       "3            1           22         59  asldeafined  train   \n",
       "4            1           24         12    aslsearch    val   \n",
       "\n",
       "                                                 url  variation_id  \\\n",
       "0       http://aslbricks.org/New/ASL-Videos/book.mp4             0   \n",
       "1  https://signstock.blob.core.windows.net/signsc...             0   \n",
       "2  https://s3-us-west-1.amazonaws.com/files.start...             0   \n",
       "3  https://media.asldeafined.com/vocabulary/14666...             0   \n",
       "4     http://www.aslsearch.com/signs/videos/book.mp4             0   \n",
       "\n",
       "     video_id  is_available  \n",
       "0  v_id_69241          True  \n",
       "1  v_id_07069          True  \n",
       "2  v_id_07068          True  \n",
       "3  v_id_07070          True  \n",
       "4  v_id_07099          True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['_id', 'source', 'url'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gloss</th>\n",
       "      <th>bbox</th>\n",
       "      <th>fps</th>\n",
       "      <th>frame_end</th>\n",
       "      <th>frame_start</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>signer_id</th>\n",
       "      <th>split</th>\n",
       "      <th>variation_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>is_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book</td>\n",
       "      <td>[385, 37, 885, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_69241</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>[462, 44, 949, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07069</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book</td>\n",
       "      <td>[234, 17, 524, 414]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07068</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>[131, 26, 526, 480]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>59</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07070</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book</td>\n",
       "      <td>[162, 54, 528, 400]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>val</td>\n",
       "      <td>0</td>\n",
       "      <td>v_id_07099</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gloss                 bbox  fps  frame_end  frame_start  instance_id  \\\n",
       "0  book  [385, 37, 885, 720]   25         -1            1            0   \n",
       "1  book  [462, 44, 949, 720]   25         -1            1           10   \n",
       "2  book  [234, 17, 524, 414]   25         -1            1           17   \n",
       "3  book  [131, 26, 526, 480]   25         -1            1           22   \n",
       "4  book  [162, 54, 528, 400]   25         -1            1           24   \n",
       "\n",
       "   signer_id  split  variation_id    video_id  is_available  \n",
       "0        118  train             0  v_id_69241          True  \n",
       "1         31  train             0  v_id_07069          True  \n",
       "2         36  train             0  v_id_07068          True  \n",
       "3         59  train             0  v_id_07070          True  \n",
       "4         12    val             0  v_id_07099          True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['video_id'] = df['video_id'].apply(lambda id: id.replace('v_id_', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gloss</th>\n",
       "      <th>bbox</th>\n",
       "      <th>fps</th>\n",
       "      <th>frame_end</th>\n",
       "      <th>frame_start</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>signer_id</th>\n",
       "      <th>split</th>\n",
       "      <th>variation_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>is_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book</td>\n",
       "      <td>[385, 37, 885, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>69241</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>[462, 44, 949, 720]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>07069</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book</td>\n",
       "      <td>[234, 17, 524, 414]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>07068</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>[131, 26, 526, 480]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>59</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>07070</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book</td>\n",
       "      <td>[162, 54, 528, 400]</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>val</td>\n",
       "      <td>0</td>\n",
       "      <td>07099</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gloss                 bbox  fps  frame_end  frame_start  instance_id  \\\n",
       "0  book  [385, 37, 885, 720]   25         -1            1            0   \n",
       "1  book  [462, 44, 949, 720]   25         -1            1           10   \n",
       "2  book  [234, 17, 524, 414]   25         -1            1           17   \n",
       "3  book  [131, 26, 526, 480]   25         -1            1           22   \n",
       "4  book  [162, 54, 528, 400]   25         -1            1           24   \n",
       "\n",
       "   signer_id  split  variation_id video_id  is_available  \n",
       "0        118  train             0    69241          True  \n",
       "1         31  train             0    07069          True  \n",
       "2         36  train             0    07068          True  \n",
       "3         59  train             0    07070          True  \n",
       "4         12    val             0    07099          True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11980"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['frame_start'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['frame_end'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'val', 'test']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['split'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8313"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['split'] == 'train').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1414"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['split'] == 'test').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2253"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['split'] == 'val').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gloss'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "    return frame_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Landmark points\n",
    "- It is a really important step which allows use to generated similar landmark pssotion values for same gestures performed in different orientation by introducing\n",
    "    - Position Invariance\n",
    "    - Scale Invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_landmarks(x , y, z , x_min , y_min , x_max , y_max , f_width , f_height):\n",
    "  \n",
    "  # Handle different types of landmark inputs\n",
    "  # if hasattr(landmarks, 'landmark'):\n",
    "  #       landmark_list = [lm for lm in landmarks.landmark]  # For face_landmarks\n",
    "  # else:\n",
    "  #       landmark_list = landmarks  # For hand landmarks that are already a list\n",
    " \n",
    "\n",
    "  normalized = []\n",
    "\n",
    "  width = x_max - x_min\n",
    "  height = y_max - y_min\n",
    "    \n",
    "  norm_x = ((x *f_width) - x_min)/(width)\n",
    "  norm_y = ((y *f_height) - y_min)/(height)\n",
    "  norm_z = z\n",
    "\n",
    "  normalized.append((norm_x , norm_y , norm_z))\n",
    "  \n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_landmarks(video_id, frame_start, frame_end , label , bb_data):\n",
    "\n",
    "#     #Creating a video path \n",
    "    \n",
    "#     video_path = rf'C:\\Users\\Sahil\\Desktop\\Talkwithhands dataset\\versions\\5\\videos\\{video_id}.mp4'\n",
    "\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#       print(f\"The video {video_id} failed to open\")\n",
    "#       return None\n",
    "    \n",
    "#     frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "#     x_min , y_min , x_max , y_max = bb_data\n",
    "    \n",
    "#     total_frames = count_frames(video_path)\n",
    "\n",
    "#     if frame_end == -1:\n",
    "#         frame_end = total_frames - 1\n",
    "\n",
    "#     landmarks_sequence = []\n",
    "    \n",
    "#     cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start)\n",
    "\n",
    "#     while cap.isOpened() and frame_start <= frame_end:\n",
    "#         current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "\n",
    "#         if current_frame > frame_end:\n",
    "#             break\n",
    "        \n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         #Crop the image using bounding box \n",
    "#         cropped_frame = frame[y_min:y_max , x_min:x_max]\n",
    "\n",
    "#         image_rgb = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         results = holistic.process(image_rgb)\n",
    "\n",
    "#         frame_landmarks = {} #stores face and hand landmarks for a frame\n",
    "\n",
    "#         if results.face_landmarks:\n",
    "#             frame_landmarks[\"face\"] = normalize_landmarks(results.face_landmarks,x_min , y_min , x_max , y_max ,frame_width , frame_height)\n",
    "#         if results.left_hand_landmarks:\n",
    "#             frame_landmarks[\"left_hand\"] = normalize_landmarks(results.left_hand_landmarks.landmark,x_min , y_min , x_max , y_max,frame_width , frame_height)\n",
    "#         if results.right_hand_landmarks:\n",
    "#             frame_landmarks[\"right_hand\"] = normalize_landmarks(results.right_hand_landmarks,x_min , y_min , x_max , y_max,frame_width , frame_height)\n",
    "        \n",
    "        \n",
    "#         landmarks_sequence.append(frame_landmarks)\n",
    "\n",
    "#     cap.release()\n",
    "\n",
    "#     #Creating a dict element for each video \n",
    "#     video_landmark_dict_element = {\n",
    "#         'landmarks' : landmarks_sequence , \n",
    "#         'label': label\n",
    "#     }\n",
    "\n",
    "\n",
    "#     return video_landmark_dict_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(video_id, frame_start, frame_end , label , bb_data):\n",
    "\n",
    "    #Creating a video path \n",
    "    \n",
    "    video_path = rf'C:\\Users\\Sahil\\Desktop\\Talkwithhands dataset\\versions\\5\\videos\\{video_id}.mp4'\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "      print(f\"The video {video_id} failed to open\")\n",
    "      return None\n",
    "    \n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(frame_width , frame_height)\n",
    "\n",
    "    x_min , y_min , x_max , y_max = bb_data\n",
    "    \n",
    "    total_frames = count_frames(video_path)\n",
    "\n",
    "    if frame_end == -1:\n",
    "        frame_end = total_frames - 1\n",
    "\n",
    "    landmarks_sequence = []\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start)\n",
    "\n",
    "    while cap.isOpened() and frame_start <= frame_end:\n",
    "        current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "\n",
    "        if current_frame > frame_end:\n",
    "            break\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        #Crop the image using bounding box \n",
    "        cropped_frame = frame[y_min:y_max , x_min:x_max]\n",
    "\n",
    "        image_rgb = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = holistic.process(image_rgb)\n",
    "\n",
    "        frame_landmarks = {} #stores face and hand landmarks for a frame\n",
    "\n",
    "        if results.face_landmarks:\n",
    "            frame_landmarks[\"face\"] = [normalize_landmarks(lm.x, lm.y, lm.z , x_min , y_min , x_max , y_max , frame_width , frame_height) for lm in results.face_landmarks.landmark]\n",
    "        if results.left_hand_landmarks:\n",
    "            frame_landmarks[\"left_hand\"] = [normalize_landmarks(lm.x, lm.y, lm.z , x_min , y_min , x_max , y_max , frame_width , frame_height) for lm in results.left_hand_landmarks.landmark]\n",
    "        if results.right_hand_landmarks:\n",
    "            frame_landmarks[\"face\"] = [normalize_landmarks(lm.x, lm.y, lm.z , x_min , y_min , x_max , y_max , frame_width , frame_height) for lm in results.right_hand_landmarks.landmark]\n",
    "        \n",
    "        \n",
    "        landmarks_sequence.append(frame_landmarks)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    #Creating a dict element for each video \n",
    "    video_landmark_dict_element = {\n",
    "        'landmarks' : landmarks_sequence , \n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "\n",
    "    return video_landmark_dict_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "d1 = extract_landmarks(df['video_id'][0] , df['frame_start'][0] , df['frame_end'][0] , df['gloss'][0] , df['bbox'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d1['landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d1['landmarks'][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11980 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#Defining an empty list to store data of each video as a dict element\n",
    "video_data = []\n",
    "\n",
    "# Applying the pre-processing function to every record\n",
    "\n",
    "df.progress_apply(lambda record : video_data.append(\n",
    " extract_landmarks(\n",
    "  video_id= record['video_id'],\n",
    "  frame_start= record['frame_start'],\n",
    "  frame_end= record['frame_end'],\n",
    "  label= record['gloss'],\n",
    "  bb_data = record['bbox']\n",
    " )\n",
    ") , axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gloss', 'bbox', 'fps', 'frame_end', 'frame_start', 'instance_id',\n",
      "       'signer_id', 'split', 'variation_id', 'video_id', 'is_available'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:   0%|          | 0/11980 [00:00<?, ?it/s]W0000 00:00:1730027111.618326  402130 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Processing videos:   2%|â–         | 224/11980 [24:37<21:32:37,  6.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m frame_end \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe_end\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./kaggle-dataset/videos/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m landmarks_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mextract_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_end\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m, in \u001b[0;36mextract_landmarks\u001b[0;34m(video_path, frame_start, frame_end)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     23\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 25\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mholistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m frame_landmarks \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mface_landmarks:\n",
      "File \u001b[0;32m~/miniconda3/envs/twh/lib/python3.12/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/twh/lib/python3.12/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# landmarks_data = []\n",
    "\n",
    "# for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing videos\"):\n",
    "#     video_id = row['video_id']\n",
    "#     frame_start = row['frame_start']\n",
    "#     frame_end = row['frame_end']\n",
    "#     video_path = f'./kaggle-dataset/videos/{video_id}.mp4'\n",
    "\n",
    "#     landmarks_sequence = extract_landmarks(video_path, frame_start, frame_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# with mp_holistic.Holistic(static_image_mode=False, \n",
    "#                            model_complexity=2, \n",
    "#                            enable_segmentation=True,\n",
    "#                            min_detection_confidence=0.5,\n",
    "#                            min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "#         ret,frame = cap.read()\n",
    "#         if not ret:\n",
    "#             print(\"Ignoring empty camera frame\")\n",
    "#             continue\n",
    "\n",
    "#         frame = cv2.flip(frame, 1)\n",
    "        \n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         image.flags.writeable = False\n",
    "\n",
    "#         results = holistic.process(image)\n",
    "\n",
    "#         image.flags.writeable = True\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#         if results.face_landmarks:\n",
    "#              mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)\n",
    "#         # if results.pose_landmarks:\n",
    "#         #      mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "#         if results.left_hand_landmarks:\n",
    "#              mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "#         if results.right_hand_landmarks:\n",
    "#              mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#         cv2.imshow('TEST', image)\n",
    "\n",
    "#         if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "#                 break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
