{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM Seperate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from tensorflow.keras.models import Model , Sequential\n",
    "from tensorflow.keras.layers import Conv2D , MaxPooling2D , Flatten , Dropout , Input , TimeDistributed , Dense , LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = MongoClient('localhost' , 27017)\n",
    "db = connection['mydb']\n",
    "collection = db['Batch_Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the data\n",
    "\n",
    "cursor = collection.find({}) \n",
    "batch_df = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>landmarks</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6729ae6b068e5f1de5e9010e</td>\n",
       "      <td>[{'face': [[0.528167724609375, 0.2578373367231...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6729ae6b068e5f1de5e9010f</td>\n",
       "      <td>[{'face': [[0.242395827902416, 0.2020864317403...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6729ae6b068e5f1de5e90110</td>\n",
       "      <td>[{'face': [[0.4664744738874764, 0.219961448490...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6729ae6b068e5f1de5e90111</td>\n",
       "      <td>[{'face': [[0.479362738886966, 0.2433891842543...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6729ae6b068e5f1de5e90112</td>\n",
       "      <td>[{'face': [[0.492517174267378, 0.0870498852922...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  6729ae6b068e5f1de5e9010e   \n",
       "1  6729ae6b068e5f1de5e9010f   \n",
       "2  6729ae6b068e5f1de5e90110   \n",
       "3  6729ae6b068e5f1de5e90111   \n",
       "4  6729ae6b068e5f1de5e90112   \n",
       "\n",
       "                                           landmarks label  \n",
       "0  [{'face': [[0.528167724609375, 0.2578373367231...  book  \n",
       "1  [{'face': [[0.242395827902416, 0.2020864317403...  book  \n",
       "2  [{'face': [[0.4664744738874764, 0.219961448490...  book  \n",
       "3  [{'face': [[0.479362738886966, 0.2433891842543...  book  \n",
       "4  [{'face': [[0.492517174267378, 0.0870498852922...  book  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_df['landmarks'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#converting the string values into numeric values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "batch_df['label_encoded'] = label_encoder.fit_transform(batch_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>landmarks</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6729ae6b068e5f1de5e9010e</td>\n",
       "      <td>[{'face': [[0.528167724609375, 0.2578373367231...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6729ae6b068e5f1de5e9010f</td>\n",
       "      <td>[{'face': [[0.242395827902416, 0.2020864317403...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6729ae6b068e5f1de5e90110</td>\n",
       "      <td>[{'face': [[0.4664744738874764, 0.219961448490...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6729ae6b068e5f1de5e90111</td>\n",
       "      <td>[{'face': [[0.479362738886966, 0.2433891842543...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6729ae6b068e5f1de5e90112</td>\n",
       "      <td>[{'face': [[0.492517174267378, 0.0870498852922...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6729ae6b068e5f1de5e9016d</td>\n",
       "      <td>[{'face': [[0.09501079164702321, 0.16571470140...</td>\n",
       "      <td>candy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6729ae6b068e5f1de5e9016e</td>\n",
       "      <td>[{'face': [[0.20198555292371412, 0.22020006891...</td>\n",
       "      <td>candy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6729ae6b068e5f1de5e9016f</td>\n",
       "      <td>[{'face': [[0.08536609051099148, 0.21951115430...</td>\n",
       "      <td>candy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6729ae6b068e5f1de5e90170</td>\n",
       "      <td>[{'face': [[0.5128556353301399, 0.295628729619...</td>\n",
       "      <td>candy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>6729ae6b068e5f1de5e90171</td>\n",
       "      <td>[{'face': [[0.482911283011114, 0.2201735847432...</td>\n",
       "      <td>candy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         _id  \\\n",
       "0   6729ae6b068e5f1de5e9010e   \n",
       "1   6729ae6b068e5f1de5e9010f   \n",
       "2   6729ae6b068e5f1de5e90110   \n",
       "3   6729ae6b068e5f1de5e90111   \n",
       "4   6729ae6b068e5f1de5e90112   \n",
       "..                       ...   \n",
       "95  6729ae6b068e5f1de5e9016d   \n",
       "96  6729ae6b068e5f1de5e9016e   \n",
       "97  6729ae6b068e5f1de5e9016f   \n",
       "98  6729ae6b068e5f1de5e90170   \n",
       "99  6729ae6b068e5f1de5e90171   \n",
       "\n",
       "                                            landmarks  label  label_encoded  \n",
       "0   [{'face': [[0.528167724609375, 0.2578373367231...   book              1  \n",
       "1   [{'face': [[0.242395827902416, 0.2020864317403...   book              1  \n",
       "2   [{'face': [[0.4664744738874764, 0.219961448490...   book              1  \n",
       "3   [{'face': [[0.479362738886966, 0.2433891842543...   book              1  \n",
       "4   [{'face': [[0.492517174267378, 0.0870498852922...   book              1  \n",
       "..                                                ...    ...            ...  \n",
       "95  [{'face': [[0.09501079164702321, 0.16571470140...  candy              2  \n",
       "96  [{'face': [[0.20198555292371412, 0.22020006891...  candy              2  \n",
       "97  [{'face': [[0.08536609051099148, 0.21951115430...  candy              2  \n",
       "98  [{'face': [[0.5128556353301399, 0.295628729619...  candy              2  \n",
       "99  [{'face': [[0.482911283011114, 0.2201735847432...  candy              2  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Makin sure that each landmark for video is a dictionary , i.e removig outer list\n",
    "\n",
    "# batch_df['landmarks'] = batch_df['landmarks'].apply(lambda x : x[0] if isinstance(x,list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_df['landmarks'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the Landmarks to align all the landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_landmarks(record):\n",
    "\n",
    " iterations = len(record)\n",
    " \n",
    " concatenated_one_video = []\n",
    "\n",
    " for count in range(iterations):\n",
    " \n",
    "    face_lands = np.array(record[count]['face'])\n",
    "    left_hand_lands = np.array(record[count]['left_hand'])\n",
    "    right_hand_lands = np.array(record[count]['right_hand'])\n",
    "\n",
    "    #Concatinating all the landmarks to the shape (510 , 3)\n",
    "\n",
    "    all_landmarks = np.vstack([face_lands , left_hand_lands , right_hand_lands]) # concatenated landmarks for 1 frame of the video\n",
    "\n",
    "    concatenated_one_video.append(all_landmarks)\n",
    " \n",
    " return np.array(concatenated_one_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = reshape_landmarks(batch_df['landmarks'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = reshape_landmarks(batch_df['landmarks'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 510, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 510, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 176.56it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_df['concatenated_landmarks'] = batch_df['landmarks'].progress_apply(reshape_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>landmarks</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "      <th>concatenated_landmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6729ae6b068e5f1de5e9010e</td>\n",
       "      <td>[{'face': [[0.528167724609375, 0.2578373367231...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0.528167724609375, 0.25783733672313774, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6729ae6b068e5f1de5e9010f</td>\n",
       "      <td>[{'face': [[0.242395827902416, 0.2020864317403...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0.242395827902416, 0.20208643174030372, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6729ae6b068e5f1de5e90110</td>\n",
       "      <td>[{'face': [[0.4664744738874764, 0.219961448490...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0.4664744738874764, 0.21996144849046956, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6729ae6b068e5f1de5e90111</td>\n",
       "      <td>[{'face': [[0.479362738886966, 0.2433891842543...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0.479362738886966, 0.2433891842543816, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6729ae6b068e5f1de5e90112</td>\n",
       "      <td>[{'face': [[0.492517174267378, 0.0870498852922...</td>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[0.492517174267378, 0.08704988529227371, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  6729ae6b068e5f1de5e9010e   \n",
       "1  6729ae6b068e5f1de5e9010f   \n",
       "2  6729ae6b068e5f1de5e90110   \n",
       "3  6729ae6b068e5f1de5e90111   \n",
       "4  6729ae6b068e5f1de5e90112   \n",
       "\n",
       "                                           landmarks label  label_encoded  \\\n",
       "0  [{'face': [[0.528167724609375, 0.2578373367231...  book              1   \n",
       "1  [{'face': [[0.242395827902416, 0.2020864317403...  book              1   \n",
       "2  [{'face': [[0.4664744738874764, 0.219961448490...  book              1   \n",
       "3  [{'face': [[0.479362738886966, 0.2433891842543...  book              1   \n",
       "4  [{'face': [[0.492517174267378, 0.0870498852922...  book              1   \n",
       "\n",
       "                              concatenated_landmarks  \n",
       "0  [[[0.528167724609375, 0.25783733672313774, -0....  \n",
       "1  [[[0.242395827902416, 0.20208643174030372, -0....  \n",
       "2  [[[0.4664744738874764, 0.21996144849046956, -0...  \n",
       "3  [[[0.479362738886966, 0.2433891842543816, -0.0...  \n",
       "4  [[[0.492517174267378, 0.08704988529227371, -0....  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 3, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((batch_df['concatenated_landmarks'][5].reshape(30 , 510 , 3 ,1))[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 510, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df['concatenated_landmarks'][5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 100174.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     (30, 510, 3)\n",
       "1     (30, 510, 3)\n",
       "2     (30, 510, 3)\n",
       "3     (30, 510, 3)\n",
       "4     (30, 510, 3)\n",
       "          ...     \n",
       "95    (30, 510, 3)\n",
       "96    (30, 510, 3)\n",
       "97    (30, 510, 3)\n",
       "98    (30, 510, 3)\n",
       "99    (30, 510, 3)\n",
       "Name: concatenated_landmarks, Length: 100, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df['concatenated_landmarks'].progress_apply(lambda row : (row).shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 99888.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#Resphaing the data to apply conv2D filter on the input data\n",
    "batch_df['concatenated_landmarks'] = batch_df['concatenated_landmarks'].progress_apply(lambda row : row.reshape(30 , 510 , 3 , 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 510, 3, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df['concatenated_landmarks'][88].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_labels = tf.keras.utils.to_categorical(batch_df['label_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(one_hot_encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data into Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = batch_df['concatenated_landmarks']\n",
    "Y = one_hot_encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 75 - 25 train - test split\n",
    "\n",
    "X_train , X_test , Y_train , Y_test = train_test_split(X , Y , test_size= 0.25 , shuffle= True , random_state= 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 30, 510, 3, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 510, 3, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Convolutional Neural Network to Extract Feature Maps for each frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping videos landmark data\n",
    "- face - 468 landmarks\n",
    "- left-hand - 21 landmarks\n",
    "- right-hand - 21 landmarks\n",
    "- to Pass t conv2D layer -> arranging the landmarks for each frame in the shape = (468 + 21 + 21 , 3) = (510 , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(input_shape = (30 , 510 , 3 , 1)):\n",
    "\n",
    " \"\"\"\n",
    " Creating a CNN model for feature extraction from individual frames\n",
    " Input shape : (timestep , landmarks , coordinates , channels)\n",
    " \"\"\"\n",
    " #Defining a Sequential Model\n",
    " model = Sequential() \n",
    "\n",
    " #Defining the Model architecture\n",
    "\n",
    " #CNN Layer 1\n",
    " model.add(TimeDistributed(Conv2D(filters= 16 , kernel_size= (3,3) , padding='same' , activation='relu' , input_shape = input_shape )))\n",
    " # model.add(TimeDistributed(MaxPooling2D((1,2))))\n",
    " model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    " #CNN Layer 2\n",
    " model.add(TimeDistributed(Conv2D(filters= 32 , kernel_size= (3,3) , padding='same' , activation='relu')))\n",
    " # model.add(TimeDistributed(MaxPooling2D((1,2))))\n",
    " model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "  #CNN Layer 3\n",
    " model.add(TimeDistributed(Conv2D(filters= 64 , kernel_size= (3,3) , padding='same' , activation='relu')))\n",
    " # model.add(TimeDistributed(MaxPooling2D((1,2))))\n",
    " model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "  #CNN Layer 4\n",
    " model.add(TimeDistributed(Conv2D(filters= 64 , kernel_size= (3,3) , padding='same' , activation='relu')))\n",
    " # model.add(TimeDistributed(MaxPooling2D((1,2))))\n",
    " # model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    " model.add(TimeDistributed(Flatten()))\n",
    "\n",
    " #Fully Connected ANN layers\n",
    " \n",
    " model.add(TimeDistributed(Dense(256 , activation = 'relu')))\n",
    " model.add(TimeDistributed(Dense(128 , activation='relu')))\n",
    "\n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting feature maps of each frame and storing it\n",
    "\n",
    "def extract_and_store_features(videos , feature_extractor):\n",
    "\n",
    " \"\"\"\n",
    " Extract Feature maps from each frame in all videos\n",
    " videos Shape : ( num_videos , 30 , 510 , 3 , 1)\n",
    " feature_extractor expects shape : (30 , 510 , 3 , 1)\n",
    " \"\"\"\n",
    "\n",
    " feature_list = []\n",
    "\n",
    " for video in videos:\n",
    "   frame_features = feature_extractor.predict(video)\n",
    "   feature_list.append(np.array(frame_features))\n",
    " \n",
    " return np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_clssifier(num_classes,feature_size = 128 , sequence_length  = 30 ):\n",
    " \"\"\"\n",
    " LSTM Model for classification using pre-extracted features from CNN \n",
    " Input Shape : (no_of_Frames , features)\n",
    " \"\"\"\n",
    "\n",
    " model = Sequential()\n",
    "\n",
    " model.add(Input(shape=(sequence_length , feature_size)))\n",
    "\n",
    " model.add(LSTM(256 , return_sequences= True))\n",
    " model.add(Dropout(0.3))\n",
    "\n",
    " model.add(LSTM(128))\n",
    " model.add(Dropout(0.3))\n",
    "\n",
    " model.add(Dense(64 , activation='relu'))\n",
    " model.add(Dense(32) ,activation = 'relu')\n",
    " model.add(Dense(num_classes , activation='softmax'))\n",
    " \n",
    " return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN_model(X_train , Y_train ,label_size, epochs = 50 , batch_size = 10):\n",
    "\n",
    " #Creating CNN model\n",
    "\n",
    " feature_extractor = CNN_model()\n",
    "\n",
    " #Extract the video Features for all videos\n",
    "\n",
    " video_features = extract_and_store_features(X_train , feature_extractor=feature_extractor)\n",
    "\n",
    " #Create and train LSTM classifier\n",
    "\n",
    " LSTM_classifier = create_LSTM_clssifier(label_size)\n",
    " LSTM_classifier.compile(optimizer = 'adam' , loss ='sparse_categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    " LSTM_model_history = LSTM_classifier.fit(video_features , Y_train , epochs = epochs , batch_size =  batch_size , validation_split = 0.2) \n",
    "\n",
    " return feature_extractor , LSTM_classifier , LSTM_model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Kernel shape must have the same length as input, but received kernel of shape (3, 3, 1, 16) and input of shape (30, 3, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m label_size \u001b[38;5;241m=\u001b[39m Y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m opt_feature_extractor , opt_LSTM_classifier , LSTM_model_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_CNN_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[74], line 9\u001b[0m, in \u001b[0;36mtrain_CNN_model\u001b[1;34m(X_train, Y_train, label_size, epochs, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m CNN_model()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Extract the video Features for all videos\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m video_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_and_store_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Create and train LSTM classifier\u001b[39;00m\n\u001b[0;32m     13\u001b[0m LSTM_classifier \u001b[38;5;241m=\u001b[39m create_LSTM_clssifier(label_size)\n",
      "Cell \u001b[1;32mIn[72], line 14\u001b[0m, in \u001b[0;36mextract_and_store_features\u001b[1;34m(videos, feature_extractor)\u001b[0m\n\u001b[0;32m     11\u001b[0m feature_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m videos:\n\u001b[1;32m---> 14\u001b[0m   frame_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m   feature_list\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(frame_features))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(feature_list)\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\operation_utils.py:184\u001b[0m, in \u001b[0;36mcompute_conv_output_shape\u001b[1;34m(input_shape, filters, kernel_size, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m    182\u001b[0m     kernel_shape \u001b[38;5;241m=\u001b[39m kernel_size \u001b[38;5;241m+\u001b[39m (input_shape[\u001b[38;5;241m1\u001b[39m], filters)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kernel_shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_shape):\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKernel shape must have the same length as input, but received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dilation_rate, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    190\u001b[0m     dilation_rate \u001b[38;5;241m=\u001b[39m (dilation_rate,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(spatial_shape)\n",
      "\u001b[1;31mValueError\u001b[0m: Kernel shape must have the same length as input, but received kernel of shape (3, 3, 1, 16) and input of shape (30, 3, 1)."
     ]
    }
   ],
   "source": [
    "label_size = Y_train.shape[1]\n",
    "\n",
    "opt_feature_extractor , opt_LSTM_classifier , LSTM_model_history = train_CNN_model(X_train , Y_train , label_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
